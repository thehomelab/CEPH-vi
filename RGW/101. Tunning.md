
## Ofical : https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments
https://ceph.com/planet/making-ceph-faster-lessons-from-performance-testing/
## 1. Hardware

![](https://i.imgur.com/2Y3mVKX.png)


## 2. MEMORY TUNING


## 3. KERNEL TUNING


Modify system control in /etc/sysctl.conf

- Tunning system trên 3 node CEPH 

```
cat <<EOF>> /etc/sysctl.conf
net.ipv4.ip_forward = 0
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_syncookies = 0
net.netfilter.nf_conntrack_max = 2621440
net.netfilter.nf_conntrack_tcp_timeout_established = 1800
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
kernel.msgmnb = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296
EOF 
```





## 3. Filesystem

- FILESYSTEM CONSIDERATIONS
Ceph is designed to be mostly filesystem agnostic–the only requirement being that the filesystem supports extended attributes (xattrs). Ceph OSDs depend on the Extended Attributes (XATTRs) of the underlying file system for: a) Internal object state b) Snapshot metadata c) RGW Access control Lists etc. Currently XFS is the recommended file system. We recommend using big inode size (default inode size is 256 bytes) when creating the file system:
- Setting the inode size is important, as XFS stores xattr data in the inode. If the metadata is too large to fit in the inode, a new extent is created, which can cause quite a performance problem. Upping the inode size to 2048 bytes provides enough room to write the default metadata, plus a little headroom.
```
mkfs.xfs –i size=2048 /dev/sda1
```

## 4. DISK READ AHEAD
Read_ahead is the file prefetching technology used in the Linux operating system. It is a system call that loads a file's contents into the page cache. When a file is subsequently accessed, its contents are read from physical memory rather than from disk, which is much faster
```
echo 2048 > /sys/block/${disk}/queue/read_ahead_kb 

echo 2048 > /sys/block/vdb/queue/read_ahead_kb 
```

- OSD: RADOS
Tuning have significant performance impact of Ceph storage system, there are hundreds of tuning knobs for swift. We will introduce some of the most important tuning setting

## 5. CFG 

- https://blog.codeship.com/linux-io-scheduler-tuning/
- 


-- Cau hinh file host
```
cat <<EOF >> /etc/hosts
192.168.50.141 ceph_node1
192.168.50.142 ceph_node2
192.168.50.143 ceph_node3
EOF
```

## cau hinh
```
cat <<EOF> ceph.conf
[global]
fsid = 35b08d01-b688-4b9a-947b-bc2e25719370
mon initial members = ceph_node1, ceph_node2, ceph_node3
mon host =  192.168.50.141, 192.168.50.142, 192.168.50.143
public_network = 192.168.50.0/24
cluster_network = 192.168.30.0/24
filestore_xattr_use_omap = true
auth_cluster_required = none
auth_service_required = none
auth_client_required = none
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_paxos = 0/0
debug_rgw = 0/0
[mon]
mon osd down out subtree limit = host
mon pg warn max object skew = 0
mon pg warn min per osd = 0
osd pool default flag hashpspool = true
osd pool default size = 3
osd pool default min size = 2
osd pool default pg num = 64
osd pool default pgp num = 64
[client]
rbd cache = false
[osd]
journal size = 20480
osd mkfs type = xfs
osd mount options xfs = rw,noatime,,nodiratime,inode64,logbsize=256k,delaylog
osd mkfs options xfs = -f -i size=2048
osd max scrubs = 1
osd scrub max interval = 4838400
osd scrub min interval = 2419200
osd deep scrub interval = 2419200
osd scrub interval randomize ratio = 1.0
osd disk thread ioprio class = idle
osd disk thread ioprio priority = 0
osd scrub chunk max = 1
osd scrub chunk min = 1
osd deep scrub stride = 1048576
osd scrub load threshold = 5.0
osd scrub sleep = 0.1
filestore_queue_max_ops=5000
filestore_queue_max_bytes = 1048576000
filestore_max_sync_interval = 10
filestore_merge_threshold = 500
filestore_split_multiple = 100
osd_op_shard_threads = 8
journal_max_write_entries = 5000
journal_max_write_bytes = 1048576000
journal_queueu_max_ops = 3000
journal_queue_max_bytes = 1048576000
ms_dispatch_throttle_bytes = 1048576000
objecter_inflight_op_bytes = 1048576000
max open files = 65536


[client.radosgw.gw2-1]
host = gw2
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw cache enabled = true
rgw cache lru size = 100000
rgw socket path = /var/run/ceph/ceph.client.radosgw.gw2-1.fastcgi.sock
rgw thread pool size = 256
rgw enable ops log = false
rgw enable usage log = false
log file = /dev/null
rgw frontends =civetweb port=80
rgw override bucket index max shards = 10
````