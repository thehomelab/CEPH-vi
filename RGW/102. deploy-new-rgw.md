
## Cài đặt Ceph ( Ceph-deploy )

## 1. Yêu cầu mà mô hình


### 1. Môi trường

- Yêu cầu phần cứng
![](images/4.png) 

- Môi trường
    - OS : Centos 7
    - User : root

### 1.2. Mô hình

- Mô hình ## chưa cập nhật 
![](images/8.png)


- Network Plan ## chưa cập nhật 
![](images/5.png)


- Lưu ý : hostname các Ceph node phải trùng với các bản ghi host .

## 2. Chuẩn bị môi trường


### 2.1. Cấu hình trên tất cả các CEPH Node và Client Node


- Cấu hình hostname
```
hostnamectl set-hostname {ceph_node1,ceph_node2,ceph_node3}
```

- Cài đặt package
```
yum install -y ntp ntpdate ntp-doc openssh-server 
yum install python-setuptools
systemctl start sshd
systemctl enable sshd
yum install -y yum-plugin-priorities 

```

-- Cau hinh file host
```
cat <<EOF > /etc/hosts
192.168.50.140 ceph_gateway   
192.168.50.132 ceph_deploy
192.168.50.141 ceph_node1
192.168.50.142 ceph_node2
192.168.50.143 ceph_node3
EOF
```

- Khởi tạo CEPH user ( sử dụng cho deploy )
```
sudo useradd -d /home/ceph_deploy -m ceph_deploy
echo "ceph_deploy:123@123Aa" | chpasswd

echo "ceph_deploy ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph_deploy
sudo chmod 0440 /etc/sudoers.d/ceph_deploy
```

- Cấu hình FirewallD
```
firewall-cmd --add-port=6789/tcp --permanent 
firewall-cmd --add-port=6800-7100/tcp --permanent
firewall-cmd --reload  
```

- Cấu hình SeLinux
```
sed -i s/^SELINUX=.*$/SELINUX=permissive/ /etc/selinux/config
setenforce 0
```

- Cấu hình NTP
```
ntpdate -qu 0.centos.pool.ntp.org 1.centos.pool.ntp.org 2.centos.pool.ntp.org
systemctl start ntpd
systemctl enable ntpd
timedatectl set-ntp true 
hwclock  -w 
```

- Trên các node CEPH, thực hiện tunning system
```
cat <<EOF>> /etc/sysctl.conf
net.ipv4.ip_forward = 0
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_syncookies = 0
net.netfilter.nf_conntrack_max = 2621440
net.netfilter.nf_conntrack_tcp_timeout_established = 1800
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
kernel.msgmnb = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296
kernel.pid_max = 4194303

EOF

sysctl -p
```





### 2.2. Cấu hình trên Node Deploy 


- Khởi tạo Repo
```
cat << EOM > /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-mimic/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
EOM
```

- Cài đặt package
```
sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
sudo yum update
sudo yum install -y ceph-deploy
```

- Cấu hình File hosts
```
cat <<EOF > /etc/hosts
192.168.50.132 ceph_deploy
192.168.50.141 ceph_node1
192.168.50.142 ceph_node2
192.168.50.143 ceph_node3
EOF
```


- Khởi tạo SSH-key trên user  `ceph_deploy` , copy sang  node2, node3
```
ssh-keygen
sudo ssh-copy-id ceph_deploy@ceph_node1
sudo ssh-copy-id ceph_deploy@ceph_node2
sudo ssh-copy-id ceph_deploy@ceph_node3
sudo ssh-copy-id ceph_deploy@ceph_gateway
```

- Cấu hình SSH config
```
$ vi  

Host ceph_node1
   Hostname ceph_node1
   User ceph_deploy
Host ceph_node2
   Hostname ceph_node2
   User ceph_deploy
Host ceph_node3
   Hostname ceph_node3
   User ceph_deploy
Host ceph_client
   Hostname ceph_gateway
   User ceph_deploy

```

## 3. Cấu hình STORAGE CLUSTER 

- Mô hình cài đặt : 1 Ceph Monitor và 3 Ceph OSD Daemons

### 3.1. Cấu hình trên node Deploy 

- Sử dụng tài khoản `root`

- Gỡ các Ceph package và các cấu hình có sẵn
```
mkdir my-cluster ## chứa cấu hình và key để access vào cụm
cd my-cluster
ceph-deploy --username ceph_deploy purge ceph_node1 ceph_node2 ceph_node3
ceph-deploy --username ceph_deploy purgedata ceph_node1 ceph_node2 ceph_node3  
ceph-deploy --username ceph_deploy forgetkeys 
rm -rf ceph.*
```



- Khởi tạo Cluster. quá trình này sẽ khởi tạo ra một mon key, và file cấu hình 
```
ceph-deploy new ceph_node1 ceph_node2 ceph_node3



[ceph_deploy.new][DEBUG ] Resolving host ceph_node3
[ceph_deploy.new][DEBUG ] Monitor ceph_node3 at 192.168.50.143
[ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph_node1', 'ceph_node2', 'ceph_node3']
[ceph_deploy.new][DEBUG ] Monitor addrs are ['192.168.50.141', '192.168.50.142', '192.168.50.143']
[ceph_deploy.new][DEBUG ] Creating a random mon key...
[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...

```

- Khởi tạo cấu hình 
```
cat <<EOF> ceph.conf

[global]
fsid = 35b08d01-b688-4b9a-947b-bc2e25719370
mon_initial_members = ceph_node1, ceph_node2, ceph_node3
mon_host = 192.168.50.141,192.168.50.142,192.168.50.143
public_network = 192.168.50.0/24
cluster_network = 192.168.30.0/24
filestore_xattr_use_omap = true
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_paxos = 0/0
debug_rgw = 0/0
[mon]
mon osd down out subtree limit = host
mon pg warn max object skew = 0
mon pg warn min per osd = 0
osd pool default flag hashpspool = true
osd pool default size = 3
osd pool default min size = 2
osd pool default pg num = 64
osd pool default pgp num = 64
mon_max_pg_per_osd = 800
[client]
rbd cache = false
[osd]
osd mkfs type = xfs
osd mount options xfs = rw,noatime,,nodiratime,inode64,logbsize=256k,delaylog
osd mkfs options xfs = -f -i size=2048
filestore max sync interval = 15
filestore min sync interval = 0.01
filestore queue commiting max ops = 5000
filestore queue max bytes = 10485760000
filestore queue max ops = 25000

osd_op_shard_threads = 8
journal_max_write_entries = 5000
journal_max_write_bytes = 1048576000
journal_queueu_max_ops = 3000
journal_queue_max_bytes = 1048576000
ms_dispatch_throttle_bytes = 1048576000
objecter_inflight_op_bytes = 1048576000
journal size = 20480
max open files = 65536

[client.rgw.gateway]
host = gateway
rgw cache enabled = true
rgw cache lru size = 100000
rgw thread pool size = 256
rgw enable ops log = false
rgw enable usage log = false
log file = /dev/null
rgw frontends =civetweb port=80
rgw override bucket index max shards = 10
EOF
```

- Cài đặt CEPH Package cho các node, sử dụng file cấu hình đã định nghĩa sẵn
```
ceph-deploy install ceph_node1 ceph_node2 ceph_node3

```

=- Khởi động MON deamon. Bước này sẽ sao chép ceph.conf sang các node MON
```
ceph-deploy mon create-initial
```

- gatherkeys sang các node 
```
ceph-deploy admin ceph_node1 ceph_node2 ceph_node3

```

- Thêm MGR
```
ceph-deploy mgr create ceph_node1 ceph_node2 ceph_node3

```

- Thêm OSD 
```
ceph-deploy osd create --data /dev/sdb ceph_node1

```

- Cài đặt radosgw
```
ceph-deploy install --rgw gateway
ceph-deploy rgw create gateway

```


- Nếu Service gateway không khởi động được thì kiểm tra `mon_max_pg_per_osd ` trên  cấu hình. Liên quan đến PG per pool default và max_pg_per_osd 