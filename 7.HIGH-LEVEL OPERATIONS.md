

## 1. HIGH-LEVEL OPERATIONS


### 1.1. Operating Cluster 


- Xem status một MON
```
systemctl status ceph-osd@id_mon
```

- Stop các Ceph daemon
```
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target

```


- Bật tất cả service
```
systemctl start ceph.target

```

- Bật service theo type daemon
```
systemctl start ceph-osd.target
systemctl start ceph-mon.target
systemctl start ceph-mds.target
```


- Xem các Ceph Service đang chạy trên node
```
systemctl status ceph\*.service ceph\*.target

```

- Khởi động tất cả các ceph service trên node
```
systemctl start ceph.target
```

- Để bật tắt service node-2 trên node1
```
systemctl -H ceph_node2 start ceph.target
```

- Xem version 
```
ceph -v
```

- Xem version của các service
```
ceph tell mon.* version
ceph tell osd.* version
```


### 1.2. Cluster Health Check 

- Kiêm tra LOG cho OSD
```
/var/log/ceph/ceph-osd.*

```


- Xem dung lượng của OSD và Pool 
```
[root@ceph_client ~]# ceph df

GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED 
    100 GiB     98 GiB      2.0 GiB          2.01 
POOLS:
    NAME         ID     USED     %USED     MAX AVAIL     OBJECTS 
    rbd_pool     1       0 B         0        31 GiB           0 

```


- Xem trạng thái của các Pool
```
cpeh osd pool stats
```

- Kiểm tra các thông số cụ thể trên các pool
```
ceph osd dump 
```

- Xem quota của các Pool
```
[root@ceph_client ~]# ceph df detail
GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED     OBJECTS 
    100 GiB     98 GiB      2.0 GiB          2.01          0  
POOLS:
    NAME         ID     QUOTA OBJECTS     QUOTA BYTES     USED     %USED     MAX AVAIL     OBJECTS     DIRTY     READ     WRITE     RAW USED 
    rbd_pool     1      N/A               N/A              0 B         0        31 GiB           0        0       0 B       0 B          0 B 
```


- Cấu hình quota
```
ceph osd pool set-quota <poolname> max_objects <num-objects>
ceph osd pool set-quota <poolname> max_bytes <num-bytes>

ceph osd pool set-quota rbd_pool max_bytes 1024

```

- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


### 1.3.Monitoring  Cluster


- CEPH cluster event bao gồm :WARN, INFO, ERROR
```
ceph -w --watch-debug
ceph -w --watch-info
ceph -w --watch-sec
ceph -w --watch-warn
ceph -w --watch-error
```


- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


- Xem log của cả cụm, dữ liệu được xuất từ /etc/ceph/ceph.loh
```
ceph -wc
```

- Kiểm tra dữ liệu được replicate và mở rộng trên các pool
```
ceph df 
```

- Kiểm tra Ratio
```
ceph osd dump | grep full_ratio

```

- Hiểm thị các ODS dưới dạng tree
```
osd osd tree
```

- Kiểm tra trạng thái các MON
```
osd mon stat 

ceph mon dump

```

- Để truy trì qourum giữa các CEPH MON, cluster yêu cầu  có nhiều hơn n/2 MON ( số mon chẵn), và ( n + 1) / 2 (mon lẻ) ở trạng thái sẵn sàng trong Cluser. 

- Kiểm tra quorum status của cluster
```
ceph quorum_status -f json-pretty
```

- Kiểm trang mạng thái MDS
```
ceph mds stat
```

- Kiểm tra trạng thái MDS cluster
```
ceph fs dump 
```


### 1.4. MONITORING OSDS AND PGS

- Ceph data placement giúp các dữ liệu được truyền vào cụm không bị rằng buộc một địa chỉ trực tiếp trên các OSD chungs được đặt trên.  Việc theo dõi dữ liệu trên cả cụm sẽ liên quan đến OSD và PGS là chủ yếu./ 


- Một OSD sẽ bao gồm 2 trạng thái trọng cụm in - the cluster và out - the cluster , bên cạnh đó sẽ có 2 trạng thái service up running và down - not running. Trong trường hợp OSD đi vào trạng thái out khỏi cụm, Ceph sẽ di dời placement groups sang OSD khác. 

- Xem trạng thái cơ bản của các OSD
```
ceph osd stat

```


- List OSD ỏ trạng thái up
```
ceph osd status
```

- Dump OSD Content
```
ceph osd dump

```

- List tất cả các OSD đang có trong CRUSH MAP
```
ceph osd tree
```

- Khởi động OSD ở trạng thái down
```
systemctl start ceph-osd@1
```

- Khi CRUSH thực hiện chỉ định  placement group cho các OSD, nó sẽ xem xét số repicate của pool, và gán các placement cho các OSD sao cho mỗi bảo sao của placement group được gán trên các OSD khác nhau. Ví dụ nếu pool yêu cầu 3 bản replica cho các placment group, CRUSH sẽ chỉ định chúng vào osd.1, osd.2 and osd3. 


- Xem trạng thái cơ bản của các pg
```
ceph pg stat

```

- Liệt kê các placement group
```
ceph pg dump

```


- Các PG sẽ bao gồm list cách trạng thái, diễn các hoạt động cũng như trạng thái của chúng trong cụm
    - Creating : PG đang được khởi tạo
    - Peering : thực hiện thoả thuận giữa các OSD, xem trạng thái của các object  trên các PG 
    - Active : sau khi thực hiện thành công quá trình peering, CEPH sẽ trả về trạng thái "active", các data trên các PG đã ở trạng thái sẵn sàng và thực hiện thành công việc replicate 
    - Clean : có nghĩa primany và secandary OSD đã thực hiện thành công quá trình peering và các PG ở trạng thái ổn định. 
    - Down : số replica tối thiểu của PG không được đáp ứng, PG đi vào trạng thái down 
    - Degraded : khi một OSD đi vào trạng thái down, quá trình chờ trạng thái OSD trở về up , PG sẽ đi vào trạng thái Degraded. Nếu sai 300s, các OSD không trở về trạng thái in, ceph sẽ thực hiện khôi phục PG sang một OSD , đảm bảo số replica, sau khi peering thành công, PG trở về trạng thái bình thường.  (Active + Clean )
    - Recovering : khi một OSD trở về trạng thái down , sau 300s, quá trình Degraded sẽ được chuyển về Recovering ,  thực hiện replica các object trong PG
    - Backfilling : sau khi một OSD mới dược thêm vào cụm, CEPhj sẽ cố gắng thực hiện cân bằng tải lại hệ thống bằng cash di chuyển một số PG sang OSD mới này  . 
    - Remapped : quá trình thực hiện migration object từ OSD cũ sang OSD mới , PG sẽ được chuyển vào trạng thái remappled
    - Stale : CEPH OSD thực hiện gửi các state của chúng tới MON định kỳ 0.5s, nếu primary OSD của PG chuyển  state "fail" ( do các OSD khác report hoặc không gửi state lên MON), các PG sẽ được set status Stale
    - Inconsistent : nếu khi các object trong quá trình replicate không đảm bảo tính toàn vẹn, PG sẽ bị trả về trạng thái Inconsistent
    - Undersized : số bản copy ít hơn số pool_size
    - Incompleted : quá trình copy không đảm bảo, gián đoạn trong quá trình write 
    - Snaptrim && snaptrim_wait :



- List các PG đang ở trạng thái stuck
```
ceph pg dump_stuck unclean

```


- http://docs.ceph.com/docs/mimic/rados/operations/monitoring-osd-pg/


- Xem vị trí của một object
```
ceph osd map {poolname} {object-name} [namespace]

rados -p data ls

```

- Xem OSD usage
```
ceph osd df 
```





### 1.5 : USER MANAGEMENT

- Khi Ceph chạy ở mode yêu cầu xác thực, nếu không chỉ định user. `client.admin ` sẽ được chỉ định là user mặc định. và sẽ sử dụng `keyring` trong /etc/ceph nếu không chỉ định keyring.

```
ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health

```

- Pool được chỉ ddinhj trong Ceph để thực hiện viết, đọc dữ liệu không bất bể Ceph Client nào. Các user được ủy nhiệm đặc quyền sẽ được ceph cho phép làm việc với các pool này. và dữ liệu trong nó. Với ceph khái niệm người dùng được quản lý được xếp vào "client". Form user trong CEPH : TYPE.ID


- Ceph sử dụng "capabilities" để thể hiện các quyền được mà các user có thể làm việc với các MON, OSD, metadata server. 
```
{daemon-type} '{cap-spec}[, {cap-spec} ...]'

```

- http://docs.ceph.com/docs/mimic/rados/operations/user-management/#background


- Xem user trên cluster
```
ceph auth ls

```

- Xóa một user
```
ceph auth del  {TYPE}.{ID}
```

- Khởi tạo kho chứa key
```
ceph-authtool -C /etc/ceph/ceph.keyring
```


- Khởi tạo 1 user
```
ceph-authtool -C /etc/ceph/ceph.client.nguyenhungsync.keyring -n client.nguyenhungsync --cap osd 'allow *'  --cap mon 'allow *' --cap mds 'allow *' --cap mgr 'allow *' --gen-key
ceph auth add client.nguyenhungsync -i /etc/ceph/ceph.client.nguyenhungsync.keyring
```


- Xem các cap của user

```
ceph auth get client.admin

```


## 2 . DATA PLACEMENT OVERVIEW

- Ceph lưu trữ và replicate các data object trên một  cụm  một cách linh động và có kế hoạch : 
    - Pools : ceph lưu trữ dữ liệu trong các pool, chứa các logical group. Pool quản lý các placment group , số lượng replicate, và CRUSH Rule cho pool . Để lưu trữ dữ liệu vào pool cần thao tác truyên người dùng được ủy quyền.
    - Placement Groups: CEPH sẽ không mapping location cụ thể các object trên các ODS, thay vi đó sẽ mapping vào placement group ( PG ). Placement sẽ đặt các object các object cùng 1 nhóm . 
    - CRUSH Maps : sẽ giảm hiện tượng nghẽn cổ chai dựa vào thuật toán cruss, cung cấp khả năng xác định nơi lưu dữ liệu ( PD group), cách sao lưu dữ liệu và nơi các bản ghi  sao chép sẽ được lưu trữ


- Việc khởi tạo 1 pool mới tương đương với việc khởi tạo một I/O interface cho các client để lưu trữ dữ liệu .Ceph client  làm việc với Cluster rất đơn giản : bắt tay và kết nối với cụm, khởi tạo I/O tuy thuộc vào reading và wrinting để lưu trữ object và các thuộct tính bổ sung . Để kết nối tới Cluster storage, cacs client cần yêu cầu có ít nhất 3 trường : "name_cluster"  + "mon_address" và secret key - user cho việc xác thực. 
![](images/19.png)



- Khi khởi tạo một cluster mới, Ceph sẽ sử dụng default pool  để lưu trữ dữ liệu . Một pool cung có thể cung cấp các khả năng sau đây :
    - Resilience : chỉ định số OSD có thể ở status fail ở mode replicas . Ví dụ với   replicated pools sẽ xác định số lượng số OSD = 3 , tương ứng với số bản copy = 3, nêu số ODS ở status fail nhỏ hơn 2, pool sẽ ngừng hoạt động. 
    - Placement Groups : chỉ số lượng placement groups  cho pool 
    - CRUSH Rules: khi đặt dữ liệu vào một rule , vị trí của dữ liệu và bản sao trong cụm được điều chỉnh dựa vào các rule
    - Snapshots : khả năng snapshot nhanh 1 pool.
    - Quotas: cung cấp khả năng giới hạn số byte cũng như số object có thể lưu trữ trên pool .

    

- Xem các pool đang có
```
[root@ceph_node1 ~]# ceph osd lspools
1 rbd_pool
2 pool_block
```

- Cấu hình số PG và PGD mặc định
```
osd pool default pg num = 100
osd pool default pgp num = 100
```

- Khởi tạo pool mới
```
ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \
     [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num}  {pgp-num}   erasure \
     [erasure-code-profile] [crush-rule-name] [expected_num_objects]

ceph osd pool create pool_block 100 100  replicated

```
- Trong đó : 
    - {pool-name} : tên của pool
    - {pg-num} :  tổng số placment group của group ( bao gồm số placement group được sử dụng và không sử dụng)
    - {pgp-num} : tổng số placement groups được sử dụng  (thông thường bằng tổng số pg-num )
    - {replicated|erasure} : replicated - cho phép các pool phục hồi object bằng cách lưu trữ nhiều bản sao cảu Object. erasure cho phép thực hiện cơ chế  RAID5 cho các object. [](https://vi.wikipedia.org/wiki/RAID#/media/File:RAID_5.svg)
    - crush-rule-name : crush rule được sử dụng cho pool
    - [expected-num-objects]: số object trên pool





- Gắn Pool vào một application. Pool cần phải được liên kết với một ứng dụng trước khi sử dụng. Ví dụ với pool được gắn vào RBD sẽ được khởi tạo bằng rbd tool. Cho phép pool cấm vận các client không được acces vào pool type. CephFS uses the application name "cephfs", RBD uses the application name "rbd", and RGW uses the application name "rgw".
```
ceph osd pool application enable {pool-name} {application-name}


ceph osd pool application enable pool_block rbd

```


- Pool quota
```
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]


ceph osd pool set-quota pool_block max_objects 100 ## set

ceph osd pool set-quota pool_block max_objects 0  ## unset 
```

- Để xoá một pool, cần cấu hình trên MON cho phép xoá pool
```
ceph tell mon.* injectargs --mon-allow-pool-delete=true ## allow 

ceph tell mon.* injectargs --mon-allow-pool-delete=false ## denied


```

- Thực hiện xoá Pool
```
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]



[]# ceph osd pool delete pool_block pool_block --yes-i-really-really-mean-it
pool 'pool_block' removed


```


- Đổi tên Pool
```
ceph osd pool rename {current-pool-name} {new-pool-name}

ceph osd pool rename {current-pool-name} {new-pool-name}

[]# ceph osd pool rename rbd_pool rbd_pool_1
pool 'rbd_pool' renamed to 'rbd_pool_1'

```

- Xem thông kê Pool
```
rados df
``` 

- Tạo snapshot cho pool
```
ceph osd pool mksnap {pool-name} {snap-name}


[]# ceph osd pool mksnap rbd_pool_1 snap_1
created pool rbd_pool_1 snap_1

```

- Xoá một snap trên pool
```
ceph osd pool rmsnap {pool-name} {snap-name}


[]# ceph osd pool rmsnap rbd_pool_1 snap_1
```

- Set Pool Value 
```
ceph osd pool set {pool-name} {key} {value}


ceph osd pool set rbd_pool_1 size 3
ceph osd pool set rbd_pool_1 min_size 2


```

- Một số key value . 

|Key|Value|
|---|---|
| compression_algorithm| `lz4`, `snappy`, `zlib`, `zstd`|
|compression_mode|`none`,  `passive`,  `aggressive`,  `force`|
|compression_min_blob_size| số byte của object tối thiểu cần thực hiện nén|
|compression_max_blob_size|byte ( interger )|
|size|số replica|
|min_size| số replicat tối thiểu|
|pg_num| số placment group|
|pgp_num| số placment group được sử dụng |
|crush_rule| crush rule áp dụng cho các placment|
|Xem thêm |http://docs.ceph.com/docs/mimic/rados/operations/pools/#set-pool-values|

- Xem value của Pool
```
ceph osd pool get {pool-name} {key}

[]# ceph osd pool get rbd_pool_1 size

```

- Xem các cấu hình của Pool
```
ceph osd dump | grep "rbd_pool_1"

```


- Xem pool detail
```
ceph osd pool ls detail 
```


## 3.  Erasure Code Pools


- Tuỳ thuộc vào yêu cầu đồ bền của dữ liệu việc xác định cách lưu trữ dữ liệu có thể được lựa chọn. Đồ bền của dữ liệu có nghĩa là khả năng đáp ứng dữ liệu mà không có mất mát trong trường hợp mất hoặc nhiều hơn OSD.
- Ceph lưu trữ dữ liệu trên các pool và sẽ có 2 kiểu pool có thể lựa chọn :
    - replicated
    - erasure-coded

- Ceph sử dụng pool replicated làm mặc định cho các object, có nghĩa rằng các bản copy của dữ liệu có thể trải đề trên các OSD. 


- erasure-coded pool đáp ứng nhu cầu bên vững nhu cầu bền vững dữ liệu hơn replicated với thụât toán riêng, sử dụng nhiều diskspace đồng thời tăng khả năng bền vững. 


- https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/storage_strategies_guide/index#erasure_code_poolsfa

## 5. PLACEMENT GROUPS

- Tính số PG dựa vào số OSD
    - Less than 5 OSDs set pg_num to 128
    - Between 5 and 10 OSDs set pg_num to 512
    - Between 10 and 50 OSDs set pg_num to 1024
    - If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself
    - For calculating pg_num value by yourself please take help of pgcalc tool



### 4.1 : HOW ARE PLACEMENT GROUPS USED ?

- Một hệ thống lớn với số lượng lớn các object sẽ không thể theo dõi từng object thay vì đó, Ceph chia nhỏ các pool bằng các placement group , gắn từng object riêng lẻ cho tầng các placement group , các placement group sẽ được gắn vào OSD chính . Nếu một OSD đi vào trạng thái fail. cluster sẽ tự động cân bằng lại bằng các thực hiện replicate các placement group, với pool size bằng 2 thì placment sẽ cố gắng copy các oject trong các placement group 2 lần .

![](images/22.png)
- Khi CRUSH thực hiện gán placemtn group  tới một OSD, nó sẽ tìm kiếm các OSD phù hợp và xác định OSD primary. Primary OSD thực hiện sử dụng CRUSH để tìm kiếm các secondary OSD  sau đó thực hiện copy  placement group’s contents . 


![](images/23.png)
- Khi primary thi vào trạng thái fail, CRUSH sẽ thực hiện chuyeẻn đổi primary OSD, thực hiện quyền nắm giữ các placement group và thực hiện sao chép các object sang OSD mới. 


### 4.1 : Data Durability

- Pending

### 4.2 :  Data Distribution within a pool

- Các object trong pool được phân bố đều trên các placement group. Khi CRUSH thực hiện  tính toán placement group cho mỗi object, CRUSH sẽ không biết được thật sự có bao nhiêu data đã được lưu trữ sẵn dưới các OSD, việc này được quản lý bởi "ratio", tỉ lệ giữa số placement group và cố OSD sẽ ảnh hưởng đến việc phân phối dữ liệu

- Ví dụ : trên một pool có replica size 3 chứa một placement group có 10 OSD, trong trường hợp này CRUSH sẽ chỉ sử dụng đến 3 OSD. Khi số placement group được thêm, các object sẽ được trải trều trên các OSD. CRUSH sẽ thực hiện chia để các data trên các OSD của các placement group . 


- Pending


### 4.3 : CHOOSING THE NUMBER OF PLACEMENT GROUPS

- Pending

