
- https://openstack-tage.de/wp-content/uploads/2016/07/DOST-2016-OpenStack-at-99.999-availability-with-Ceph-slide.pdf



## 1. 
Ceph delivers object, block and file storage on one platform, delivering:
    ● Scalability from petabytes to exabytes
    ● High Availability--hardware failure is an expectation, not just an exception
    ○ Data durability: replication or erasure coding
    ○ Data distribution: Data is evenly and pseudo-randomly distributed
    ○ Fault tolerance: Ability to confine failures. No single point of failure.
    ○ Resilience: Automatic recovery from a degraded state

Ceph is a very appealing cloud storage solution for OpenStack.


## 2. 

Sometimes the term “software-defined storage” leads to misconceptions.
    ● “Commodity hardware” doesn’t mean old or low performance hardware
    ● Monitors are lightweight, but they are mission critical.
    ● The gateway supports multiple performance profiles--with different hardware
    ● Ceph (like all distributed systems) relies on networking.
    ● Saving money on hardware may increase expenses elsewhere


## 3. Hardware plan

![](https://i.imgur.com/Omnv4ra.png)


## 4. Network Plan for HA

![](https://i.imgur.com/ONWkioQ.png)


## 4. Network Plan for production

![](https://i.imgur.com/mQVIf7e.png)


## 5. Storage Plan

![](https://i.imgur.com/ooWme7K.png)


![](https://i.imgur.com/k5Gtoo9.png)


## 5. CEPH PLan for production

![](https://i.imgur.com/ZJ1lcpO.png)

## 6. Small Cluster Deployment


![](https://i.imgur.com/vWtoJsM.png)


## 7. Ceph Balancer

### 7.1 Intro and Enable Ceph Balancer 
- The balancer plugin can optimize the placement of PGs across OSDs in order to achieve a balanced distribution, either automatically or in a supervised fashion.


![](http://docs.ceph.com/docs/mimic/mgr/balancer/)

```
ceph mgr module enable balancer
ceph balancer on
ceph balancer mode upmap
ceph balancer status

```

There are currently two supported balancer modes:
    - crush-compat ( default ). The CRUSH compat mode uses the compat weight-set feature (introduced in Luminous) to manage an alternative set of weights for devices in the CRUSH hierarchy. The normal weights should remain set to the size of the device to reflect the target amount of data that we want to store on the device. The balancer then optimizes the weight-set values, adjusting them up or down in small increments, in order to achieve a distribution that matches the target distribution as closely as possible. (Because PG placement is a pseudorandom process, there is a natural amount of variation in the placement; by optimizing the weights we counter-act that natural variation.)Notably, this mode is fully backwards compatible with older clients: when an OSDMap and CRUSH map is shared with older clients, we present the optimized weights as the “real” weights.he primary restriction of this mode is that the balancer cannot handle multiple CRUSH hierarchies with different placement rules if the subtrees of the hierarchy share any OSDs. (This is normally not the case, and is generally not a recommended configuration because it is hard to manage the space utilization on the shared OSDs.)

    - upmap. Starting with Luminous, the OSDMap can store explicit mappings for individual OSDs as exceptions to the normal CRUSH placement calculation. These upmap entries provide fine-grained control over the PG mapping. This CRUSH mode will optimize the placement of individual PGs in order to achieve a balanced distribution. In most cases, this distribution is “perfect,” which an equal number of PGs on each OSD (+/-1 PG, since they might not divide evenly).


The balancer operation is broken into a few distinct phases:
    - building a plan
    - evaluating the quality of the data distribution, either for the current PG distribution, or the PG distribution that would result after executing a plan
    - executing the plan


### 7.2 

- Xem điểm về cơ chế distributed hiện tại
```
ceph balancer eval
ceph balancer eval {pool}
ceph balancer verbose
```

- Khởi tạo plan và excute plan
```
ceph balancer optimize <plan-name> {<pool-name>}


ceph balancer optimize <plan> {<pool>}
ceph balancer eval <plan-name>
ceph balancer execute <plan-name>
```


## 8 

![](https://i.imgur.com/hYL4soH.png)


## 9 . Ceph FS

![](https://i.imgur.com/DpFsuzu.png)


## 10. defined

- Hyper-converged infrastructure (HCI) is a software-defined IT infrastructure that virtualizes all of the elements of conventional "hardware-defined" systems. HCI includes, at a minimum, virtualized computing (a hypervisor), a virtualised SAN (software-defined storage) and virtualized networking (software-defined networking). HCI typically runs on commercial off-the-shelf (COTS) servers. The term was first used in this context by Scale Computing in 2012 when the term was coined by Arun Taneja to describe the Scale HC3 platform.[1][2]

11. Ceph s3 with Keystone Case Issue

![](https://i.imgur.com/wLyUkxN.png)


12. Ceph Topo at Flipkart

![](https://i.imgur.com/0uKp5LE.png)


13. Ceph Block Flow for VM

![](https://i.imgur.com/0dhQnY3.png)